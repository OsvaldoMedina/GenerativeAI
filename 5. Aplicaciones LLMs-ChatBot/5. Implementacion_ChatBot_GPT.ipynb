{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Caso Práctico: Implementación de un ChatBot con GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico, se propone al alumno el desarrollo de un ChatBot que permita interactuar con los clientes de un restaturante y recopilar los pedidos de comida que realicen para entregar a domiciolio o recoger en las instalaciones.\n",
        "\n",
        "Para ello, el alumno debe implementar un progorama en Python que acceda de manera programática a los servicios de OpenAI y permita interactuar con uno de los LLMs que ofrecen.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8dgd_BUeOK"
      },
      "source": [
        "## 0. Instalación de librerías externas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ScQcgtCkUhD7"
      },
      "outputs": [],
      "source": [
        "#!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d620f6"
      },
      "source": [
        "## 1. Lectura de la API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_pc_repo = \"C:/Users/Osvaldo/OneDrive/Documents/3. Projectos Visual Studio/GenerativeAI/\"\n",
        "path_repo = \"data/data\"\n",
        "key_path = \"/5. Aplicaciones LLMs-ChatBot/\"\n",
        "file_info = \"/api-key.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pVplMtLdUx2w"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "path_file = path_pc_repo + key_path + file_info\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/api-keys/secret-key.txt\") as f:\n",
        "#  openai.api_key = f.readline()\n",
        "\n",
        "with open(path_file ) as f:\n",
        "  openai.api_key = f.readline()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNFCLMHWCXs"
      },
      "source": [
        "## 2. Selección del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP3Zs5O1WHER"
      },
      "source": [
        "En la función `obtener_completion` que habíamos implementado para casos prácticos anteriores, únicamente hacíamos uso del `role` de `user`:\n",
        "\n",
        "```python\n",
        "def obtener_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "  mensaje = [{\"role\": \"user\", \"content\": prompt}]\n",
        "  respuesta = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=mensaje,\n",
        "      temperature=0, # Este hiperparámetro controla la aleatoriedad del modelo\n",
        "  )\n",
        "  return respuesta.choices[0].message[\"content\"]\n",
        "```\n",
        "\n",
        "En este caso práctico, vamos a tener que explorar el [resto de roles que nos proporcionar OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api) para sus LLMs y la función que tienen cada uno de ellos:\n",
        "\n",
        "\n",
        "*  Rol `user`: Representa al usuario final que interactúa con el LLM a través de un chat.\n",
        "*  Rol `assistant`: Representa al LLM que estemos utilizando, en este caso, `gpt-3.5-turbo`.\n",
        "*  Rol `system`: Este rol representa al desarrollador de sistema. Permite proporcionar instrucciones \"root\" al LLM para que se sigan durante la conversación con el usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dkxOGRD-WF3p"
      },
      "outputs": [],
      "source": [
        "def obtener_completion(mensajes, model=\"gpt-3.5-turbo\"):\n",
        "  respuesta = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=mensajes,\n",
        "      temperature=0, # Este hiperparámetro controla la aleatoriedad del modelo\n",
        "  )\n",
        "  return respuesta.choices[0].message[\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej2wP15VXrPY"
      },
      "source": [
        "## 3. Predicción/Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cTEQ4sG6dwcT"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m mensajes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEres un ChatBot amigable con un conocimiento experto sobre Ciberseguridad. Responde detalladamente a las preguntas de los usuarios\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m      3\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuenos días. Acabo de recibir un SMS sospechoso. ¿Crees que puede ser phishing?. El SMS es: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimado cliente, su paquete no se ha podido entregar el 02/09 porque no se han pagado las tasas de aduanas (? 1). siga las instrucciones: http:/67m.us/9OYpR\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 5\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m \u001b[43mobtener_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmensajes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(respuesta)\n",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mobtener_completion\u001b[1;34m(mensajes, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobtener_completion\u001b[39m(mensajes, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m   respuesta \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmensajes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Este hiperparámetro controla la aleatoriedad del modelo\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m respuesta\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Osvaldo\\.conda\\envs\\GeneraAI\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
            "File \u001b[1;32mc:\\Users\\Osvaldo\\.conda\\envs\\GeneraAI\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[1;32mc:\\Users\\Osvaldo\\.conda\\envs\\GeneraAI\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
            "File \u001b[1;32mc:\\Users\\Osvaldo\\.conda\\envs\\GeneraAI\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Osvaldo\\.conda\\envs\\GeneraAI\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ],
      "source": [
        "mensajes = [\n",
        "    {'role': 'system', 'content':'Eres un ChatBot amigable con un conocimiento experto sobre Ciberseguridad. Responde detalladamente a las preguntas de los usuarios'},\n",
        "    {'role': 'user', 'content':'Buenos días. Acabo de recibir un SMS sospechoso. ¿Crees que puede ser phishing?. El SMS es: \"Estimado cliente, su paquete no se ha podido entregar el 02/09 porque no se han pagado las tasas de aduanas (? 1). siga las instrucciones: http:/67m.us/9OYpR'}\n",
        "]\n",
        "respuesta = obtener_completion(mensajes)\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAADIVDfgLKD"
      },
      "source": [
        "Si intentamos continuar una conversación manteniendo este esquema de preguntas/respuestas, nos daremos cuenta de que el LLM no preserva el contexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuWPbF01gcdn"
      },
      "outputs": [],
      "source": [
        "mensajes = [\n",
        "    {'role': 'system', 'content':'Eres un ChatBot amigable. Responde a las preguntas de los usuarios.'},\n",
        "    {'role': 'user', 'content':'Buenos días. Me llamo Santiago.'}\n",
        "]\n",
        "respuesta = obtener_completion(mensajes)\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDd5t__VgoCw"
      },
      "outputs": [],
      "source": [
        "mensajes = [\n",
        "    {'role': 'system', 'content':'Eres un ChatBot amigable. Responde a las preguntas de los usuarios.'},\n",
        "    {'role': 'user', 'content':'¿Podrías decirme de dónde proviene mi nombre?'}\n",
        "]\n",
        "respuesta = obtener_completion(mensajes)\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMIme246gBlQ"
      },
      "source": [
        "## 4. Predicción/Generación de texto preservando el contexto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TJnv3GGg4sD"
      },
      "source": [
        "Para preservar el contexto de la conversación, necesitamos almacenar las interacciones con el LLM en la lista de mensajes y proporcionárselo en las nuevas interacciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLiHb8icgCsq"
      },
      "outputs": [],
      "source": [
        "mensajes = [\n",
        "    {'role': 'system', 'content':'Eres un ChatBot amigable. Responde a las preguntas de los usuarios.'},\n",
        "    {'role': 'user', 'content':'Buenos días. Me llamo Santiago.'},\n",
        "    {'role': 'assistant', 'content':'¡Buenos días, Santiago! ¿En qué puedo ayudarte hoy?'},\n",
        "    {'role': 'user', 'content': '¿Podrías decirme de dónde proviene mi nombre?'}\n",
        "]\n",
        "respuesta = obtener_completion(mensajes)\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WplHNlNhiQP"
      },
      "source": [
        "## 5. Implementación de un ChatBot interactivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gClV6n5kfZV"
      },
      "outputs": [],
      "source": [
        "def collect_messages(_):\n",
        "    prompt = inp.value_input\n",
        "    inp.value = ''\n",
        "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
        "    response = obtener_completion(context)\n",
        "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
        "    panels.append(\n",
        "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
        "    panels.append(\n",
        "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, styles={'background-color': '#F6F6F6'})))\n",
        "    return pn.Column(*panels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DWyA21Cp5pL"
      },
      "outputs": [],
      "source": [
        "def end_chat(event):\n",
        "    panels.append(pn.pane.Alert(\"Chat terminado por el usuario.\", alert_type='success'))\n",
        "    # Solicito al LLM que me genere un JSON con toda la conversación\n",
        "    context.append({'role': 'system', 'content':\"\"\"Genera un resumen del pedido en formato JSON. La salida debe ser únicamente el JSON sin nada más. \\\n",
        "  El JSON debe incluir las siguientes claves:\n",
        "  \"pizzas\": <lista de pizzas solicitadas>\n",
        "  \"tamaño_pizzas\": <tamaño de las pizzas solicitadas>\n",
        "  \"precios_pizzas\"> <precios de las pizzas solicitadas>\n",
        "  \"ingredientes_extras\": <lista de ingredientes extra solicitados>\n",
        "  \"tamaño_extras\": <tamaño de los ingredientes extra seleccionados>\n",
        "  \"precios_extras\": <precios de los ingredientes extra solicitados>\n",
        "  \"bebidas\": <lista de bebidas solicitadas>\n",
        "  \"precios_bebidas\": <precios de las bebidas solicitadas>\n",
        "  \"direccion_entrega\", <En el caso de entrega a domicilio, direccion de entrega, en caso contrario, pones NA>\"\"\"})\n",
        "    pedido_json = obtener_completion(context)\n",
        "    print(f\"\\nEl pedido realizado es:\\n{pedido_json}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!#pip install panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0Sb46D4hgzB"
      },
      "outputs": [],
      "source": [
        "import panel as pn  # GUI\n",
        "pn.extension()\n",
        "\n",
        "panels = []\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "Eres un OrderBot, un servicio automatizado para recopilar pedidos para un restaurante de pizza. \\\n",
        "Primero saludas al cliente, luego recopilas el pedido, \\\n",
        "y luego preguntas si es para recoger o entregar. \\\n",
        "Esperas a recopilar todo el pedido, luego lo resumes y verificas una última vez \\\n",
        "si el cliente quiere agregar algo más. \\\n",
        "Si es para entrega, preguntas por una dirección. \\\n",
        "Finalmente, recopilas el pago. \\\n",
        "Asegúrate de aclarar todas las opciones, extras y tamaños para identificar de manera única \\\n",
        "el artículo del menú. \\\n",
        "Respondes de manera corta, muy amigable y conversacional. \\\n",
        "El menú incluye los siguientes productos al siguiente precio en euros:\n",
        "pizza de pepperoni 12.95, 10.00, 7.00\n",
        "pizza de queso 10.95, 9.25, 6.50\n",
        "pizza de berenjena 11.95, 9.75, 6.75\n",
        "papas fritas 4.50, 3.50\n",
        "ensalada griega 7.25\n",
        "Ingredientes extra:\n",
        "queso 2.00\n",
        "champiñones 1.50\n",
        "salchicha 3.00\n",
        "tocino canadiense 3.50\n",
        "salsa AI 1.50\n",
        "pimientos 1.00\n",
        "Bebidas:\n",
        "coca-cola 3.00, 2.00, 1.00\n",
        "sprite 3.00, 2.00, 1.00\n",
        "agua embotellada 5.00\n",
        "\"\"\"} ]\n",
        "\n",
        "\n",
        "inp = pn.widgets.TextInput(value=\"Hola\", placeholder='Introduce texto aqui...')\n",
        "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
        "button_end_chat = pn.widgets.Button(name=\"Terminar Chat\")\n",
        "\n",
        "button_end_chat.on_click(end_chat)\n",
        "\n",
        "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
        "\n",
        "dashboard = pn.Column(\n",
        "    inp,\n",
        "    pn.Row(button_conversation, button_end_chat),\n",
        "    pn.panel(interactive_conversation, loading_indicator=True, sizing_mode=\"stretch_both\"),\n",
        ")\n",
        "\n",
        "dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9a_oOX9j7FA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
